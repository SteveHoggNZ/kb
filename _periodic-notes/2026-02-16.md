# 2026-02-16

← [[2026-02-15]] | [[2026-02-17]] →

---

## KB Maintenance

Restructured notes to move related links closer to the concepts they reference, rather than having disjointed lists at the bottom.

**Files updated:**

- [[Reversibility-Calibrated Action]] — Added "The Bike Principle" section explaining the connection to [[The Collapse of Roles and Time]] and [[Rules of Thumb]] in context
- [[Rules of Thumb]] — Wove reversibility links into "On Speed" section; transformed Related Concepts into "When Heuristics Fail" with explanatory context
- [[The Collapse of Roles and Time]] — Added Rules of Thumb quote to Bike Principle section; removed redundant list (Connection to Other Concepts section already had contextual links)
- [[Teach the Delta]] — Added [[The Collapse of Roles and Time]] to Connection section
- [[Context Distillation Loop - amnesia as a feature]] — Added missing connections (Teach the Delta, Leverage Selection, The 2026 Builder Operating System, Scarcity to Abundance) to Connection section

**Pattern established:** Links belong in contextual sections that explain *why* the connection matters. Keep only minimal "See Also" for MOC navigation.

---

## Meta

Created `.claude/CLAUDE.md` to document rules for AI agents working on this KB. Key principle: links close to definitions, not at the bottom.

---

## New Notes Added

### [[concepts/ai-development/Agentation - UI Element Annotation|Agentation (UI Element Annotation)]]
Visual feedback instrument for AI coding agents — click page elements, get CSS selectors for grep searches. Bridges "that button" (visual) to code references. A tool for High-Quality Intent Specification.

### [[concepts/ai-development/Scalable Multi-Agent Architecture|Scalable Multi-Agent Architecture]]
The "human team" fallacy: mimicking human collaboration fails at scale. Five rules from Cursor/Gastown: two tiers not teams, workers stay ignorant, no shared state, plan for endings (episodic operation), sophisticated orchestration + simple agents. Creates productive tension with [[Teams Of Agents - LLM Specialisation+Personas]] — different tools for different scales.

### [[concepts/ai-development/Law vs Physics in Agent Design|Law vs Physics in Agent Design]]
Fundamental distinction for reliable agentic systems. **Law** (prompts) = voluntary, degrades under context pressure. **Physics** (API/platform) = involuntary, cannot be bypassed. Design rule: for every Law, ask "What happens when the agent ignores this?" — if catastrophic, add Physics. Extracted from AMP multi-agent coordination planning doc.

---

## Files Updated

- [[concepts/ai-development/Scalable Multi-Agent Architecture|Scalable Multi-Agent Architecture]] — Added Law vs Physics connection to existing patterns table
- [[concepts/ai-development/Agents vs Long Context|Agents vs Long Context]] — Added orchestration pattern section linking to Context Distillation Loop
- [[_MOCs/AI-Assisted Development]] — Added Agentation, Scalable Multi-Agent Architecture, Law vs Physics entries
- [[_MOCs/Design Principles]] — Added Taste in Software entry
- [[SAGE Synthesis]] — Logged all new notes
